<html>
  <head>
    <style>
      .linenums {
        list-style-type: none;
      }

      .formatted-line-numbers {
        display: none;
      }
      .action-code-block {
        display: none;
      }
      table {
        border-collapse: collapse;
        width: 100%;
      }
      table,
      th,
      td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h2>
      <span class="header-link octicon octicon-link"></span>Cheat Sheet: AI
      Models for NLP
    </h2>
    <table>
      <colgroup>
        <col style="width: 9%" />
        <col style="width: 57%" />
        <col style="width: 57%" />
      </colgroup>
      <thead>
        <tr class="header">
          <th>Package/Method</th>
          <th>Description</th>
          <th>Code example</th>
        </tr>
      </thead>
      <tbody>
        <tr class="odd">
          <td>PyTorch/Embedding and EmbeddingBag</td>
          <td>
            Embedding is a class that represents an embedding layer. It accepts
            token indices and produces embedding vectors. EmbeddingBag is a
            class that aggregates embeddings using mean or sum operations.
            Embedding and EmbeddingBag are part of the torch.nn module. The code
            example shows how you can use Embedding and EmbeddingBag in PyTorch.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li><li>14</li><li>15</li><li>16</li><li>17</li><li>18</li><li>19</li><li>20</li><li>21</li><li>22</li><li>23</li><li>24</li><li>25</li><li>26</li><li>27</li><li>28</li><li>29</li><li>30</li><li>31</li><li>32</li><li>33</li><li>34</li><li>35</li><li>36</li><li>37</li><li>38</li></ol><ol class="linenums"><li class="L0"><span class="com"># Defining a data set</span></li><li class="L1"><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[</span></li><li class="L2"><span class="str">"I like cats"</span><span class="pun">,</span></li><li class="L3"><span class="str">"I hate dogs"</span><span class="pun">,</span></li><li class="L4"><span class="str">"I'm impartial to hippos"</span></li><li class="L5"><span class="pun">]</span></li><li class="L6"><span class="com">#Initializing the tokenizer, iterator from the data set, and vocabulary</span></li><li class="L7"><span class="pln">tokenizer </span><span class="pun">=</span><span class="pln"> get_tokenizer</span><span class="pun">(</span><span class="str">'spacy'</span><span class="pun">,</span><span class="pln"> language</span><span class="pun">=</span><span class="str">'en_core_web_sm'</span><span class="pun">)</span></li><li class="L8"><span class="kwd">def</span><span class="pln"> yield_tokens</span><span class="pun">(</span><span class="pln">data_iter</span><span class="pun">):</span></li><li class="L9"><span class="pln">    </span><span class="kwd">for</span><span class="pln"> data_sample </span><span class="kwd">in</span><span class="pln"> data_iter</span><span class="pun">:</span></li><li class="L0"><span class="pln">        </span><span class="kwd">yield</span><span class="pln"> tokenizer</span><span class="pun">(</span><span class="pln">data_sample</span><span class="pun">)</span></li><li class="L1"><span class="pln">data_iter </span><span class="pun">=</span><span class="pln"> iter</span><span class="pun">(</span><span class="pln">dataset</span><span class="pun">)</span></li><li class="L2"><span class="pln">vocab </span><span class="pun">=</span><span class="pln"> build_vocab_from_iterator</span><span class="pun">(</span><span class="pln">yield_tokens</span><span class="pun">(</span><span class="pln">data_iter</span><span class="pun">))</span></li><li class="L3"><span class="com">#Tokenizing and generating indices</span></li><li class="L4"><span class="pln">input_ids</span><span class="pun">=</span><span class="kwd">lambda</span><span class="pln"> x</span><span class="pun">:[</span><span class="pln">torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">(</span><span class="pln">tokenizer</span><span class="pun">(</span><span class="pln">data_sample</span><span class="pun">)))</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> data_sample </span><span class="kwd">in</span><span class="pln"> dataset</span><span class="pun">]</span></li><li class="L5"><span class="pln">index</span><span class="pun">=</span><span class="pln">input_ids</span><span class="pun">(</span><span class="pln">dataset</span><span class="pun">)</span></li><li class="L6"><span class="kwd">print</span><span class="pun">(</span><span class="pln">index</span><span class="pun">)</span></li><li class="L7"><span class="com">#Initiating the embedding layer, specifying the dimension size for the embeddings, </span></li><li class="L8"><span class="com">#determining the count of unique tokens present in the vocabulary, and creating the embedding layer</span></li><li class="L9"><span class="pln">embedding_dim </span><span class="pun">=</span><span class="pln"> </span><span class="lit">3</span></li><li class="L0"><span class="pln">n_embedding </span><span class="pun">=</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">)</span></li><li class="L1"><span class="pln">n_embedding</span><span class="pun">:</span><span class="lit">9</span></li><li class="L2"><span class="pln">embeds </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Embedding</span><span class="pun">(</span><span class="pln">n_embedding</span><span class="pun">,</span><span class="pln"> embedding_dim</span><span class="pun">)</span></li><li class="L3"><span class="com">#Applying the embedding object</span></li><li class="L4"><span class="pln">i_like_cats</span><span class="pun">=</span><span class="pln">embeds</span><span class="pun">(</span><span class="pln">index</span><span class="pun">[</span><span class="lit">0</span><span class="pun">])</span></li><li class="L5"><span class="pln">i_like_cats</span></li><li class="L6"><span class="pln">impartial_to_hippos</span><span class="pun">=</span><span class="pln">embeds</span><span class="pun">(</span><span class="pln">index</span><span class="pun">[-</span><span class="lit">1</span><span class="pun">])</span></li><li class="L7"><span class="pln">impartial_to_hippos</span></li><li class="L8"><span class="com">#Initializing the embedding bag layer</span></li><li class="L9"><span class="pln">embedding_dim </span><span class="pun">=</span><span class="pln"> </span><span class="lit">3</span></li><li class="L0"><span class="pln">n_embedding </span><span class="pun">=</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">)</span></li><li class="L1"><span class="pln">n_embedding</span><span class="pun">:</span><span class="lit">9</span></li><li class="L2"><span class="pln">embedding_bag </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">EmbeddingBag</span><span class="pun">(</span><span class="pln">n_embedding</span><span class="pun">,</span><span class="pln"> embedding_dim</span><span class="pun">)</span></li><li class="L3"><span class="com"># Output the embedding bag</span></li><li class="L4"><span class="pln">dataset </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[</span><span class="str">"I like cats"</span><span class="pun">,</span><span class="str">"I hate dogs"</span><span class="pun">,</span><span class="str">"I'm impartial to hippos"</span><span class="pun">]</span></li><li class="L5"><span class="pln">index</span><span class="pun">:[</span><span class="pln">tensor</span><span class="pun">([</span><span class="lit">0</span><span class="pun">,</span><span class="pln"> </span><span class="lit">7</span><span class="pun">,</span><span class="pln"> </span><span class="lit">2</span><span class="pun">]),</span><span class="pln"> tensor</span><span class="pun">([</span><span class="lit">0</span><span class="pun">,</span><span class="pln"> </span><span class="lit">4</span><span class="pun">,</span><span class="pln"> </span><span class="lit">3</span><span class="pun">]),</span><span class="pln"> tensor</span><span class="pun">([</span><span class="lit">0</span><span class="pun">,</span><span class="pln"> </span><span class="lit">1</span><span class="pun">,</span><span class="pln"> </span><span class="lit">6</span><span class="pun">,</span><span class="pln"> </span><span class="lit">8</span><span class="pun">,</span><span class="pln"> </span><span class="lit">5</span><span class="pun">])]</span></li><li class="L6"><span class="pln">i_like_cats</span><span class="pun">=</span><span class="pln">embedding_bag</span><span class="pun">(</span><span class="pln">index</span><span class="pun">[</span><span class="lit">0</span><span class="pun">],</span><span class="pln">offsets</span><span class="pun">=</span><span class="pln">torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">([</span><span class="lit">0</span><span class="pun">]))</span></li><li class="L7"><span class="pln">i_like_cats</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-0">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>Batch function</td>
          <td>
            Defines the number of samples that will be propagated through the
            network.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li></ol><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln"> collate_batch</span><span class="pun">(</span><span class="pln">batch</span><span class="pun">):</span></li><li class="L1"><span class="pln">target_list</span><span class="pun">,</span><span class="pln"> context_list</span><span class="pun">,</span><span class="pln"> offsets </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[],</span><span class="pln"> </span><span class="pun">[],</span><span class="pln"> </span><span class="pun">[</span><span class="lit">0</span><span class="pun">]</span></li><li class="L2"><span class="kwd">for</span><span class="pln"> _context</span><span class="pun">,</span><span class="pln"> _target </span><span class="kwd">in</span><span class="pln"> batch</span><span class="pun">:</span></li><li class="L3"><span class="pln">target_list</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">[</span><span class="pln">_target</span><span class="pun">])</span><span class="pln"> </span></li><li class="L4"><span class="pln">processed_context </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">(</span><span class="pln">text_pipeline</span><span class="pun">(</span><span class="pln">_context</span><span class="pun">),</span><span class="pln"> dtype</span><span class="pun">=</span><span class="pln">torch</span><span class="pun">.</span><span class="pln">int64</span><span class="pun">)</span></li><li class="L5"><span class="pln">context_list</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">processed_context</span><span class="pun">)</span></li><li class="L6"><span class="pln">offsets</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">processed_context</span><span class="pun">.</span><span class="pln">size</span><span class="pun">(</span><span class="lit">0</span><span class="pun">))</span></li><li class="L7"><span class="pln">target_list </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">(</span><span class="pln">target_list</span><span class="pun">,</span><span class="pln"> dtype</span><span class="pun">=</span><span class="pln">torch</span><span class="pun">.</span><span class="pln">int64</span><span class="pun">)</span></li><li class="L8"><span class="pln">offsets </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">(</span><span class="pln">offsets</span><span class="pun">[:-</span><span class="lit">1</span><span class="pun">]).</span><span class="pln">cumsum</span><span class="pun">(</span><span class="pln">dim</span><span class="pun">=</span><span class="lit">0</span><span class="pun">)</span></li><li class="L9"><span class="pln">context_list </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">cat</span><span class="pun">(</span><span class="pln">context_list</span><span class="pun">)</span></li><li class="L0"><span class="kwd">return</span><span class="pln"> target_list</span><span class="pun">.</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">),</span><span class="pln"> context_list</span><span class="pun">.</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">),</span><span class="pln"> offsets</span><span class="pun">.</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">)</span></li><li class="L1"><span class="pln">BATCH_SIZE </span><span class="pun">=</span><span class="pln"> </span><span class="lit">64</span><span class="pln"> </span><span class="com"># batch size for training</span></li><li class="L2"><span class="pln">dataloader_cbow </span><span class="pun">=</span><span class="pln"> </span><span class="typ">DataLoader</span><span class="pun">(</span><span class="pln">cobw_data</span><span class="pun">,</span><span class="pln"> batch_size</span><span class="pun">=</span><span class="pln">BATCH_SIZE</span><span class="pun">,</span><span class="pln"> shuffle</span><span class="pun">=</span><span class="kwd">True</span><span class="pun">,</span><span class="pln"> collate_fn</span><span class="pun">=</span><span class="pln">collate_batch</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-1">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Forward pass</td>
          <td>
            Refers to the computation and storage of intermediate variables
            (including outputs) for a neural network in order from the input to
            the output layer.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style="padding-right: 42px"
            ><ol class="formatted-line-numbers"><li>1</li></ol><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln"> forward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> text</span><span class="pun">):</span></li></ol><button title="Copy" class="action-code-block copy-code-block one-line"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-2">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>Stanford's pre-trained GloVe</td>
          <td>
            Leverages large-scale data for word embeddings. It can be integrated
            into PyTorch for improved NLP tasks such as classification.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li></ol><ol class="linenums"><li class="L0"><span class="kwd">from</span><span class="pln"> torchtext</span><span class="pun">.</span><span class="pln">vocab </span><span class="kwd">import</span><span class="pln"> </span><span class="typ">GloVe</span><span class="pun">,</span><span class="pln">vocab</span></li><li class="L1"><span class="com"># Creating an instance of the 6B version of Glove() model</span></li><li class="L2"><span class="pln">glove_vectors_6B </span><span class="pun">=</span><span class="pln"> </span><span class="typ">GloVe</span><span class="pun">(</span><span class="pln">name </span><span class="pun">=</span><span class="str">'6B'</span><span class="pun">)</span><span class="pln"> </span><span class="com"># you can specify the model with the following format: GloVe(name='840B', dim=300)</span></li><li class="L3"><span class="com"># Build vocab from glove_vectors</span></li><li class="L4"><span class="pln">vocab </span><span class="pun">=</span><span class="pln"> vocab</span><span class="pun">(</span><span class="pln">glove_vectors_6B</span><span class="pun">.</span><span class="pln">stoi</span><span class="pun">,</span><span class="pln"> </span><span class="lit">0</span><span class="pun">,</span><span class="pln">specials</span><span class="pun">=(</span><span class="str">'&lt;unk&gt;'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'&lt;pad&gt;'</span><span class="pun">))</span></li><li class="L5"><span class="pln">vocab</span><span class="pun">.</span><span class="pln">set_default_index</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">[</span><span class="str">"&lt;unk&gt;"</span><span class="pun">])</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-3">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>vocab</td>
          <td>
            The vocab object is part of the PyTorch torchtext library. It maps
            tokens to indices. The code example shows how you can apply the
            vocab object to tokens directly.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li></ol><ol class="linenums"><li class="L0"><span class="com"># Takes an iterator as input and extracts the next tokenized sentence. Creates a list of token indices using the vocab dictionary for each token.</span></li><li class="L1"><span class="kwd">def</span><span class="pln"> get_tokenized_sentence_and_indices</span><span class="pun">(</span><span class="pln">iterator</span><span class="pun">):</span></li><li class="L2"><span class="pln">    tokenized_sentence </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">next</span><span class="pun">(</span><span class="pln">iterator</span><span class="pun">)</span></li><li class="L3"><span class="pln">    token_indices </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[</span><span class="pln">vocab</span><span class="pun">[</span><span class="pln">token</span><span class="pun">]</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> token </span><span class="kwd">in</span><span class="pln"> tokenized_sentence</span><span class="pun">]</span></li><li class="L4"><span class="pln">    </span><span class="kwd">return</span><span class="pln"> tokenized_sentence</span><span class="pun">,</span><span class="pln"> token_indices</span></li><li class="L5"><span class="com"># Returns the tokenized sentences and the corresponding token indices. Repeats the process.</span></li><li class="L6"><span class="pln">tokenized_sentence</span><span class="pun">,</span><span class="pln"> token_indices </span><span class="pun">=</span><span class="pln"> get_tokenized_sentence_and_indices</span><span class="pun">(</span><span class="pln">my_iterator</span><span class="pun">)</span></li><li class="L7"><span class="kwd">next</span><span class="pun">(</span><span class="pln">my_iterator</span><span class="pun">)</span></li><li class="L8"><span class="com"># Prints the tokenized sentence and its corresponding token indices.</span></li><li class="L9"><span class="kwd">print</span><span class="pun">(</span><span class="str">"Tokenized Sentence:"</span><span class="pun">,</span><span class="pln"> tokenized_sentence</span><span class="pun">)</span></li><li class="L0"><span class="kwd">print</span><span class="pun">(</span><span class="str">"Token Indices:"</span><span class="pun">,</span><span class="pln"> token_indices</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-4">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Special tokens in PyTorch: &lt;eos&gt; and &lt;bos&gt;</td>
          <td>
            Tokens introduced to input sequences to convey specific information
            or serve a particular purpose during training. The code example
            shows the use of &lt;bos&gt; and &lt;eos&gt; during tokenization.
            The &lt;bos&gt; token denotes the beginning of the input sequence,
            and the &lt;eos&gt; token denotes the end.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li></ol><ol class="linenums"><li class="L0"><span class="com"># Appends &lt;bos&gt; at the beginning and &lt;eos&gt; at the end of the tokenized sentences </span></li><li class="L1"><span class="com"># using a loop that iterates over the sentences in the input data</span></li><li class="L2"><span class="pln">tokenizer_en </span><span class="pun">=</span><span class="pln"> get_tokenizer</span><span class="pun">(</span><span class="str">'spacy'</span><span class="pun">,</span><span class="pln"> language</span><span class="pun">=</span><span class="str">'en_core_web_sm'</span><span class="pun">)</span></li><li class="L3"><span class="pln">tokens </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[]</span></li><li class="L4"><span class="pln">max_length </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0</span></li><li class="L5"><span class="kwd">for</span><span class="pln"> line </span><span class="kwd">in</span><span class="pln"> lines</span><span class="pun">:</span></li><li class="L6"><span class="pln">    tokenized_line </span><span class="pun">=</span><span class="pln"> tokenizer_en</span><span class="pun">(</span><span class="pln">line</span><span class="pun">)</span></li><li class="L7"><span class="pln">    tokenized_line </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[</span><span class="str">'&lt;bos&gt;'</span><span class="pun">]</span><span class="pln"> </span><span class="pun">+</span><span class="pln"> tokenized_line </span><span class="pun">+</span><span class="pln"> </span><span class="pun">[</span><span class="str">'&lt;eos&gt;'</span><span class="pun">]</span></li><li class="L8"><span class="pln">    tokens</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">tokenized_line</span><span class="pun">)</span></li><li class="L9"><span class="pln">    max_length </span><span class="pun">=</span><span class="pln"> max</span><span class="pun">(</span><span class="pln">max_length</span><span class="pun">,</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">tokenized_line</span><span class="pun">))</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-5">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>Special tokens in PyTorch: &lt;pad&gt;</td>
          <td>
            Tokens introduced to input sequences to convey specific information
            or serve a particular purpose during training. The code example
            shows the use of &lt;pad&gt; token to ensure all sentences have the
            same length.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li></ol><ol class="linenums"><li class="L0"><span class="com"># Pads the tokenized lines</span></li><li class="L1"><span class="kwd">for</span><span class="pln"> i </span><span class="kwd">in</span><span class="pln"> range</span><span class="pun">(</span><span class="pln">len</span><span class="pun">(</span><span class="pln">tokens</span><span class="pun">)):</span></li><li class="L2"><span class="pln">    tokens</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> tokens</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span><span class="pln"> </span><span class="pun">+</span><span class="pln"> </span><span class="pun">[</span><span class="str">'&lt;pad&gt;'</span><span class="pun">]</span><span class="pln"> </span><span class="pun">*</span><span class="pln"> </span><span class="pun">(</span><span class="pln">max_length </span><span class="pun">-</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">tokens</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]))</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-6">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Cross entropy loss</td>
          <td>
            A metric used in machine learning (ML) to evaluate the performance
            of a classification model. The loss is measured as the probability
            value between 0 (perfect model) and 1. Typically, the aim is to
            bring the model as close to 0 as possible.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li></ol><ol class="linenums"><li class="L0"><span class="kwd">from</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">nn </span><span class="kwd">import</span><span class="pln"> </span><span class="typ">CrossEntropyLoss</span></li><li class="L1"><span class="pln">model </span><span class="pun">=</span><span class="pln"> </span><span class="typ">TextClassificationModel</span><span class="pun">(</span><span class="pln">vocab_size</span><span class="pun">,</span><span class="pln">emsize</span><span class="pun">,</span><span class="pln">num_class</span><span class="pun">)</span></li><li class="L2"><span class="pln">loss_fn </span><span class="pun">=</span><span class="pln"> </span><span class="typ">CrossEntropyLoss</span><span class="pun">()</span></li><li class="L3"><span class="pln">predicted_label </span><span class="pun">=</span><span class="pln"> model</span><span class="pun">(</span><span class="pln">text</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">)</span></li><li class="L4"><span class="pln">loss </span><span class="pun">=</span><span class="pln"> criterion</span><span class="pun">(</span><span class="pln">predicted_label</span><span class="pun">,</span><span class="pln"> label</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-7">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>Optimization</td>
          <td>Method to reduce losses in a model.</td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li></ol><ol class="linenums"><li class="L0"><span class="com"># Creates an iterator object</span></li><li class="L1"><span class="pln">optimizer </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">optim</span><span class="pun">.</span><span class="pln">SGD</span><span class="pun">(</span><span class="pln">model</span><span class="pun">.</span><span class="pln">parameters</span><span class="pun">(),</span><span class="pln"> lr</span><span class="pun">=</span><span class="lit">0.1</span><span class="pun">)</span></li><li class="L2"><span class="pln">scheduler </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">optim</span><span class="pun">.</span><span class="pln">lr_scheduler</span><span class="pun">.</span><span class="typ">StepLR</span><span class="pun">(</span><span class="pln">optimizer</span><span class="pun">,</span><span class="pln"> </span><span class="lit">1.0</span><span class="pun">,</span><span class="pln"> gamma</span><span class="pun">=</span><span class="lit">0.1</span><span class="pun">)</span></li><li class="L3"><span class="pln">optimizer</span><span class="pun">.</span><span class="pln">zero_grad</span><span class="pun">()</span></li><li class="L4"><span class="pln">predicted_label </span><span class="pun">=</span><span class="pln"> model</span><span class="pun">(</span><span class="pln">text</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">)</span></li><li class="L5"><span class="pln">loss </span><span class="pun">=</span><span class="pln"> criterion</span><span class="pun">(</span><span class="pln">predicted_label</span><span class="pun">,</span><span class="pln"> label</span><span class="pun">)</span></li><li class="L6"><span class="pln">loss</span><span class="pun">.</span><span class="pln">backward</span><span class="pun">()</span></li><li class="L7"><span class="pln">torch</span><span class="pun">.</span><span class="pln">nn</span><span class="pun">.</span><span class="pln">utils</span><span class="pun">.</span><span class="pln">clip_grad_norm_</span><span class="pun">(</span><span class="pln">model</span><span class="pun">.</span><span class="pln">parameters</span><span class="pun">(),</span><span class="pln"> </span><span class="lit">0.1</span><span class="pun">)</span></li><li class="L8"><span class="pln">optimizer</span><span class="pun">.</span><span class="pln">step</span><span class="pun">()</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-8">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>sentence_bleu()</td>
          <td>
            NLTK (or Natural Language Toolkit) provides this function to
            evaluate a hypothesis sentence against one or more reference
            sentences. The reference sentences must be presented as a list of
            sentences where each reference is a list of tokens.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li></ol><ol class="linenums"><li class="L0"><span class="kwd">from</span><span class="pln"> nltk</span><span class="pun">.</span><span class="pln">translate</span><span class="pun">.</span><span class="pln">bleu_score </span><span class="kwd">import</span><span class="pln"> sentence_bleu</span></li><li class="L1"><span class="kwd">def</span><span class="pln"> calculate_bleu_score</span><span class="pun">(</span><span class="pln">generated_translation</span><span class="pun">,</span><span class="pln"> reference_translations</span><span class="pun">):</span></li><li class="L2"><span class="com"># Convert the generated translations and reference translations into the expected format for sentence_bleu</span></li><li class="L3"><span class="pln">references </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[</span><span class="pln">reference</span><span class="pun">.</span><span class="pln">split</span><span class="pun">()</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> reference </span><span class="kwd">in</span><span class="pln"> reference_translations</span><span class="pun">]</span></li><li class="L4"><span class="pln">hypothesis </span><span class="pun">=</span><span class="pln"> generated_translation</span><span class="pun">.</span><span class="pln">split</span><span class="pun">()</span></li><li class="L5"><span class="com"># Calculate the BLEU score</span></li><li class="L6"><span class="pln">bleu_score </span><span class="pun">=</span><span class="pln"> sentence_bleu</span><span class="pun">(</span><span class="pln">references</span><span class="pun">,</span><span class="pln"> hypothesis</span><span class="pun">)</span></li><li class="L7"><span class="kwd">return</span><span class="pln"> bleu_score</span></li><li class="L8"><span class="pln">reference_translations </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[</span><span class="str">"Asian man sweeping the walkway ."</span><span class="pun">,</span><span class="str">"An asian man sweeping the walkway ."</span><span class="pun">,</span><span class="str">"An Asian man sweeps the sidewalk ."</span><span class="pun">,</span><span class="str">"An Asian man is sweeping the sidewalk ."</span><span class="pun">,</span><span class="str">"An asian man is sweeping the walkway ."</span><span class="pun">,</span><span class="str">"Asian man sweeping the sidewalk ."</span><span class="pun">]</span></li><li class="L9"><span class="pln">bleu_score </span><span class="pun">=</span><span class="pln"> calculate_bleu_score</span><span class="pun">(</span><span class="pln">generated_translation</span><span class="pun">,</span><span class="pln"> reference_translations</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-9">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Encoder RNN model</td>
          <td>
            The encoder-decoder seq2seq model works together to transform an
            input sequence into an output sequence. Encoder is a series of RNNs
            that process the input sequence individually, passing their hidden
            states to their next RNN.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li></ol><ol class="linenums"><li class="L0"><span class="kwd">class</span><span class="pln"> </span><span class="typ">Encoder</span><span class="pun">(</span><span class="pln">nn</span><span class="pun">.</span><span class="typ">Module</span><span class="pun">):</span></li><li class="L1"><span class="kwd">def</span><span class="pln"> __init__</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> vocab_len</span><span class="pun">,</span><span class="pln"> emb_dim</span><span class="pun">,</span><span class="pln"> hid_dim</span><span class="pun">,</span><span class="pln"> n_layers</span><span class="pun">,</span><span class="pln"> dropout_prob</span><span class="pun">):</span></li><li class="L2"><span class="kwd">super</span><span class="pun">().</span><span class="pln">__init__</span><span class="pun">()</span></li><li class="L3"><span class="kwd">self</span><span class="pun">.</span><span class="pln">hid_dim </span><span class="pun">=</span><span class="pln"> hid_dim</span></li><li class="L4"><span class="kwd">self</span><span class="pun">.</span><span class="pln">n_layers </span><span class="pun">=</span><span class="pln"> n_layers</span></li><li class="L5"><span class="kwd">self</span><span class="pun">.</span><span class="pln">embedding </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Embedding</span><span class="pun">(</span><span class="pln">vocab_len</span><span class="pun">,</span><span class="pln"> emb_dim</span><span class="pun">)</span></li><li class="L6"><span class="kwd">self</span><span class="pun">.</span><span class="pln">lstm </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="pln">LSTM</span><span class="pun">(</span><span class="pln">emb_dim</span><span class="pun">,</span><span class="pln"> hid_dim</span><span class="pun">,</span><span class="pln"> n_layers</span><span class="pun">,</span><span class="pln"> dropout </span><span class="pun">=</span><span class="pln"> dropout_prob</span><span class="pun">)</span></li><li class="L7"><span class="kwd">self</span><span class="pun">.</span><span class="pln">dropout </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Dropout</span><span class="pun">(</span><span class="pln">dropout_prob</span><span class="pun">)</span></li><li class="L8"><span class="kwd">def</span><span class="pln"> forward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> input_batch</span><span class="pun">):</span></li><li class="L9"><span class="pln">embed </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">dropout</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">embedding</span><span class="pun">(</span><span class="pln">input_batch</span><span class="pun">))</span></li><li class="L0"><span class="pln">embed </span><span class="pun">=</span><span class="pln"> embed</span><span class="pun">.</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">)</span></li><li class="L1"><span class="pln">outputs</span><span class="pun">,</span><span class="pln"> </span><span class="pun">(</span><span class="pln">hidden</span><span class="pun">,</span><span class="pln"> cell</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">lstm</span><span class="pun">(</span><span class="pln">embed</span><span class="pun">)</span></li><li class="L2"><span class="kwd">return</span><span class="pln"> hidden</span><span class="pun">,</span><span class="pln"> cell</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-10">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>Decoder RNN model</td>
          <td>
            The encoder-decoder seq2seq model works together to transform an
            input sequence into an output sequence. The decoder module is a
            series of RNNs that autoregressively generates the translation as
            one token at a time. Each generated token goes back into the next
            RNN along with the hidden state to generate the next token of the
            output sequence until the end token is generated.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li><li>14</li><li>15</li><li>16</li><li>17</li><li>18</li></ol><ol class="linenums"><li class="L0"><span class="kwd">class</span><span class="pln"> </span><span class="typ">Decoder</span><span class="pun">(</span><span class="pln">nn</span><span class="pun">.</span><span class="typ">Module</span><span class="pun">):</span></li><li class="L1"><span class="kwd">def</span><span class="pln"> __init__</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> output_dim</span><span class="pun">,</span><span class="pln"> emb_dim</span><span class="pun">,</span><span class="pln"> hid_dim</span><span class="pun">,</span><span class="pln"> n_layers</span><span class="pun">,</span><span class="pln"> dropout</span><span class="pun">):</span></li><li class="L2"><span class="kwd">super</span><span class="pun">().</span><span class="pln">__init__</span><span class="pun">()</span></li><li class="L3"><span class="kwd">self</span><span class="pun">.</span><span class="pln">output_dim </span><span class="pun">=</span><span class="pln"> output_dim</span></li><li class="L4"><span class="kwd">self</span><span class="pun">.</span><span class="pln">hid_dim </span><span class="pun">=</span><span class="pln"> hid_dim</span></li><li class="L5"><span class="kwd">self</span><span class="pun">.</span><span class="pln">n_layers </span><span class="pun">=</span><span class="pln"> n_layers</span></li><li class="L6"><span class="kwd">self</span><span class="pun">.</span><span class="pln">embedding </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Embedding</span><span class="pun">(</span><span class="pln">output_dim</span><span class="pun">,</span><span class="pln"> emb_dim</span><span class="pun">)</span></li><li class="L7"><span class="kwd">self</span><span class="pun">.</span><span class="pln">lstm </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="pln">LSTM</span><span class="pun">(</span><span class="pln">emb_dim</span><span class="pun">,</span><span class="pln"> hid_dim</span><span class="pun">,</span><span class="pln"> n_layers</span><span class="pun">,</span><span class="pln"> dropout </span><span class="pun">=</span><span class="pln"> dropout</span><span class="pun">)</span></li><li class="L8"><span class="kwd">self</span><span class="pun">.</span><span class="pln">fc_out </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Linear</span><span class="pun">(</span><span class="pln">hid_dim</span><span class="pun">,</span><span class="pln"> output_dim</span><span class="pun">)</span></li><li class="L9"><span class="kwd">self</span><span class="pun">.</span><span class="pln">softmax </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">LogSoftmax</span><span class="pun">(</span><span class="pln">dim</span><span class="pun">=</span><span class="lit">1</span><span class="pun">)</span></li><li class="L0"><span class="kwd">self</span><span class="pun">.</span><span class="pln">dropout </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Dropout</span><span class="pun">(</span><span class="pln">dropout</span><span class="pun">)</span></li><li class="L1"><span class="kwd">def</span><span class="pln"> forward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> input</span><span class="pun">,</span><span class="pln"> hidden</span><span class="pun">,</span><span class="pln"> cell</span><span class="pun">):</span></li><li class="L2"><span class="pln">input </span><span class="pun">=</span><span class="pln"> input</span><span class="pun">.</span><span class="pln">unsqueeze</span><span class="pun">(</span><span class="lit">0</span><span class="pun">)</span></li><li class="L3"><span class="pln">embedded </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">dropout</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">.</span><span class="pln">embedding</span><span class="pun">(</span><span class="pln">input</span><span class="pun">))</span></li><li class="L4"><span class="pln">output</span><span class="pun">,</span><span class="pln"> </span><span class="pun">(</span><span class="pln">hidden</span><span class="pun">,</span><span class="pln"> cell</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">lstm</span><span class="pun">(</span><span class="pln">embedded</span><span class="pun">,</span><span class="pln"> </span><span class="pun">(</span><span class="pln">hidden</span><span class="pun">,</span><span class="pln"> cell</span><span class="pun">))</span></li><li class="L5"><span class="pln">prediction_logit </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">fc_out</span><span class="pun">(</span><span class="pln">output</span><span class="pun">.</span><span class="pln">squeeze</span><span class="pun">(</span><span class="lit">0</span><span class="pun">))</span></li><li class="L6"><span class="pln">prediction </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">softmax</span><span class="pun">(</span><span class="pln">prediction_logit</span><span class="pun">)</span></li><li class="L7"><span class="kwd">return</span><span class="pln"> prediction</span><span class="pun">,</span><span class="pln"> hidden</span><span class="pun">,</span><span class="pln"> cell</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-11">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Skip-gram model</td>
          <td>
            Predicts surrounding context words from a specific target word. It
            predicts one context word at a time from a target word.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li><li>14</li><li>15</li><li>16</li><li>17</li><li>18</li><li>19</li><li>20</li><li>21</li><li>22</li><li>23</li><li>24</li><li>25</li><li>26</li><li>27</li></ol><ol class="linenums"><li class="L0"><span class="kwd">class</span><span class="pln"> </span><span class="typ">SkipGram_Model</span><span class="pun">(</span><span class="pln">nn</span><span class="pun">.</span><span class="typ">Module</span><span class="pun">):</span></li><li class="L1"><span class="kwd">def</span><span class="pln"> __init__</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> vocab_size</span><span class="pun">,</span><span class="pln"> embed_dim</span><span class="pun">):</span></li><li class="L2"><span class="kwd">super</span><span class="pun">(</span><span class="typ">SkipGram_Model</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">).</span><span class="pln">__init__</span><span class="pun">()</span></li><li class="L3"><span class="com"># Define the embeddings layer</span></li><li class="L4"><span class="kwd">self</span><span class="pun">.</span><span class="pln">embeddings </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Embedding</span><span class="pun">(</span><span class="pln">num_embeddings</span><span class="pun">=</span><span class="pln">vocab_size</span><span class="pun">,</span><span class="pln"> embedding_dim</span><span class="pun">=</span><span class="pln">embed_dim</span><span class="pun">)</span></li><li class="L5"><span class="com"># Define the fully connected layer</span></li><li class="L6"><span class="kwd">self</span><span class="pun">.</span><span class="pln">fc </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Linear</span><span class="pun">(</span><span class="pln">in_features</span><span class="pun">=</span><span class="pln">embed_dim</span><span class="pun">,</span><span class="pln"> out_features</span><span class="pun">=</span><span class="pln">vocab_size</span><span class="pun">)</span></li><li class="L7"><span class="com"># Perform the forward pass</span></li><li class="L8"><span class="kwd">def</span><span class="pln"> forward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> text</span><span class="pun">):</span></li><li class="L9"><span class="com"># Pass the input text through the embeddings layer</span></li><li class="L0"><span class="kwd">out</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">embeddings</span><span class="pun">(</span><span class="pln">text</span><span class="pun">)</span></li><li class="L1"><span class="com"># Pass the output of the embeddings layer through the fully connected layer</span></li><li class="L2"><span class="com"># Apply the ReLU activation function</span></li><li class="L3"><span class="kwd">out</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">relu</span><span class="pun">(</span><span class="kwd">out</span><span class="pun">)</span></li><li class="L4"><span class="kwd">out</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">fc</span><span class="pun">(</span><span class="kwd">out</span><span class="pun">)</span></li><li class="L5"><span class="pln">    </span><span class="kwd">return</span><span class="pln"> </span><span class="kwd">out</span></li><li class="L6"><span class="pln">model_sg </span><span class="pun">=</span><span class="pln"> </span><span class="typ">SkipGram_Model</span><span class="pun">(</span><span class="pln">vocab_size</span><span class="pun">,</span><span class="pln"> emsize</span><span class="pun">).</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">)</span></li><li class="L7"><span class="com"># Sequence generation function</span></li><li class="L8"><span class="pln">CONTEXT_SIZE </span><span class="pun">=</span><span class="pln"> </span><span class="lit">2</span></li><li class="L9"><span class="pln">skip_data </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[]</span></li><li class="L0"><span class="kwd">for</span><span class="pln"> i </span><span class="kwd">in</span><span class="pln"> range</span><span class="pun">(</span><span class="pln">CONTEXT_SIZE</span><span class="pun">,</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">tokenized_toy_data</span><span class="pun">)</span><span class="pln"> </span><span class="pun">-</span><span class="pln"> CONTEXT_SIZE</span><span class="pun">):</span></li><li class="L1"><span class="pln">    context </span><span class="pun">=</span><span class="pln"> </span><span class="pun">(</span></li><li class="L2"><span class="pln">    </span><span class="pun">[</span><span class="pln">tokenized_toy_data</span><span class="pun">[</span><span class="pln">i </span><span class="pun">-</span><span class="pln"> j </span><span class="pun">-</span><span class="pln"> </span><span class="lit">1</span><span class="pun">]</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> j </span><span class="kwd">in</span><span class="pln"> range</span><span class="pun">(</span><span class="pln">CONTEXT_SIZE</span><span class="pun">)]</span><span class="pln"> </span><span class="com"># Preceding words</span></li><li class="L3"><span class="pln">    </span><span class="pun">+</span><span class="pln"> </span><span class="pun">[</span><span class="pln">tokenized_toy_data</span><span class="pun">[</span><span class="pln">i </span><span class="pun">+</span><span class="pln"> j </span><span class="pun">+</span><span class="pln"> </span><span class="lit">1</span><span class="pun">]</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> j </span><span class="kwd">in</span><span class="pln"> range</span><span class="pun">(</span><span class="pln">CONTEXT_SIZE</span><span class="pun">)]</span><span class="pln"> </span><span class="com"># Succeeding words)</span></li><li class="L4"><span class="pln">    target </span><span class="pun">=</span><span class="pln"> tokenized_toy_data</span><span class="pun">[</span><span class="pln">i</span><span class="pun">]</span></li><li class="L5"><span class="pln">    skip_data</span><span class="pun">.</span><span class="pln">append</span><span class="pun">((</span><span class="pln">target</span><span class="pun">,</span><span class="pln"> context</span><span class="pun">))</span></li><li class="L6"><span class="pln">skip_data</span><span class="pun">=[(</span><span class="str">'i'</span><span class="pun">,</span><span class="pln"> </span><span class="pun">[</span><span class="str">'wish'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'i'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'was'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'little'</span><span class="pun">]),</span><span class="pln"> </span><span class="pun">(</span><span class="str">'was'</span><span class="pun">,</span><span class="pln"> </span><span class="pun">[</span><span class="str">'i'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'wish'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'little'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'bit'</span><span class="pun">])],..</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-12">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>collate_fn</td>
          <td>
            Processes the list of samples to form a batch. The
            <code>batch</code> argument is a list of all your samples.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li></ol><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln"> collate_fn</span><span class="pun">(</span><span class="pln">batch</span><span class="pun">):</span></li><li class="L1"><span class="pln">    target_list</span><span class="pun">,</span><span class="pln"> context_list </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[],</span><span class="pln"> </span><span class="pun">[]</span></li><li class="L2"><span class="pln">    </span><span class="kwd">for</span><span class="pln"> _context</span><span class="pun">,</span><span class="pln"> _target </span><span class="kwd">in</span><span class="pln"> batch</span><span class="pun">:</span></li><li class="L3"><span class="pln">        target_list</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">[</span><span class="pln">_target</span><span class="pun">])</span></li><li class="L4"><span class="pln">        context_list</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">[</span><span class="pln">_context</span><span class="pun">])</span></li><li class="L5"><span class="pln">        target_list </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">(</span><span class="pln">target_list</span><span class="pun">,</span><span class="pln"> dtype</span><span class="pun">=</span><span class="pln">torch</span><span class="pun">.</span><span class="pln">int64</span><span class="pun">)</span></li><li class="L6"><span class="pln">        context_list </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">tensor</span><span class="pun">(</span><span class="pln">context_list</span><span class="pun">,</span><span class="pln"> dtype</span><span class="pun">=</span><span class="pln">torch</span><span class="pun">.</span><span class="pln">int64</span><span class="pun">)</span></li><li class="L7"><span class="pln">    </span><span class="kwd">return</span><span class="pln"> target_list</span><span class="pun">.</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">),</span><span class="pln"> context_list</span><span class="pun">.</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-13">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Training function</td>
          <td>
            Trains the model for a specified number of epochs. It also includes
            a condition to check whether the input is for skip-gram or CBOW. The
            output of this function includes the trained model and a list of
            average losses for each epoch.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li><li>14</li><li>15</li><li>16</li><li>17</li><li>18</li><li>19</li><li>20</li><li>21</li><li>22</li><li>23</li><li>24</li><li>25</li></ol><ol class="linenums"><li class="L0"><span class="kwd">def</span><span class="pln"> train_model</span><span class="pun">(</span><span class="pln">model</span><span class="pun">,</span><span class="pln"> dataloader</span><span class="pun">,</span><span class="pln"> criterion</span><span class="pun">,</span><span class="pln"> optimizer</span><span class="pun">,</span><span class="pln"> num_epochs</span><span class="pun">=</span><span class="lit">1000</span><span class="pun">):</span></li><li class="L1"><span class="com"># List to store running loss for each epoch</span></li><li class="L2"><span class="pln">epoch_losses </span><span class="pun">=</span><span class="pln"> </span><span class="pun">[]</span></li><li class="L3"><span class="kwd">for</span><span class="pln"> epoch </span><span class="kwd">in</span><span class="pln"> tqdm</span><span class="pun">(</span><span class="pln">range</span><span class="pun">(</span><span class="pln">num_epochs</span><span class="pun">)):</span></li><li class="L4"><span class="pln">    </span><span class="com"># Storing running loss values for the current epoch</span></li><li class="L5"><span class="pln">running_loss </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0.0</span></li><li class="L6"><span class="com"># Using tqdm for a progress bar</span></li><li class="L7"><span class="kwd">for</span><span class="pln"> idx</span><span class="pun">,</span><span class="pln"> samples </span><span class="kwd">in</span><span class="pln"> enumerate</span><span class="pun">(</span><span class="pln">dataloader</span><span class="pun">):</span></li><li class="L8"><span class="pln">optimizer</span><span class="pun">.</span><span class="pln">zero_grad</span><span class="pun">()</span></li><li class="L9"><span class="com"># Check for EmbeddingBag layer in the model CBOW</span></li><li class="L0"><span class="kwd">if</span><span class="pln"> any</span><span class="pun">(</span><span class="pln">isinstance</span><span class="pun">(</span><span class="kwd">module</span><span class="pun">,</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">EmbeddingBag</span><span class="pun">)</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> _</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">module</span><span class="pln"> </span><span class="kwd">in</span><span class="pln"> model</span><span class="pun">.</span><span class="pln">named_modules</span><span class="pun">()):</span></li><li class="L1"><span class="pln">target</span><span class="pun">,</span><span class="pln"> context</span><span class="pun">,</span><span class="pln"> offsets </span><span class="pun">=</span><span class="pln"> samples</span></li><li class="L2"><span class="pln">predicted </span><span class="pun">=</span><span class="pln"> model</span><span class="pun">(</span><span class="pln">context</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">)</span></li><li class="L3"><span class="com"># Check for Embedding layer in the model skip gram</span></li><li class="L4"><span class="kwd">elif</span><span class="pln"> any</span><span class="pun">(</span><span class="pln">isinstance</span><span class="pun">(</span><span class="kwd">module</span><span class="pun">,</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Embedding</span><span class="pun">)</span><span class="pln"> </span><span class="kwd">for</span><span class="pln"> _</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">module</span><span class="pln"> </span><span class="kwd">in</span><span class="pln"> model</span><span class="pun">.</span><span class="pln">named_modules</span><span class="pun">()):</span></li><li class="L5"><span class="pln">target</span><span class="pun">,</span><span class="pln"> context </span><span class="pun">=</span><span class="pln"> samples</span></li><li class="L6"><span class="pln">predicted </span><span class="pun">=</span><span class="pln"> model</span><span class="pun">(</span><span class="pln">context</span><span class="pun">)</span></li><li class="L7"><span class="pln">loss </span><span class="pun">=</span><span class="pln"> criterion</span><span class="pun">(</span><span class="pln">predicted</span><span class="pun">,</span><span class="pln"> target</span><span class="pun">)</span></li><li class="L8"><span class="pln">loss</span><span class="pun">.</span><span class="pln">backward</span><span class="pun">()</span></li><li class="L9"><span class="pln">torch</span><span class="pun">.</span><span class="pln">nn</span><span class="pun">.</span><span class="pln">utils</span><span class="pun">.</span><span class="pln">clip_grad_norm_</span><span class="pun">(</span><span class="pln">model</span><span class="pun">.</span><span class="pln">parameters</span><span class="pun">(),</span><span class="pln"> </span><span class="lit">0.1</span><span class="pun">)</span></li><li class="L0"><span class="pln">optimizer</span><span class="pun">.</span><span class="pln">step</span><span class="pun">()</span></li><li class="L1"><span class="pln">running_loss </span><span class="pun">+=</span><span class="pln"> loss</span><span class="pun">.</span><span class="pln">item</span><span class="pun">()</span></li><li class="L2"><span class="com"># Append average loss for the epoch</span></li><li class="L3"><span class="pln">epoch_losses</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">running_loss </span><span class="pun">/</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">dataloader</span><span class="pun">))</span></li><li class="L4"><span class="kwd">return</span><span class="pln"> model</span><span class="pun">,</span><span class="pln"> epoch_losses</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-14">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="even">
          <td>CBOW model</td>
          <td>
            Utilizes context words to predict a target word and generate its
            embedding.
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li><li>14</li><li>15</li><li>16</li><li>17</li><li>18</li></ol><ol class="linenums"><li class="L0"><span class="kwd">class</span><span class="pln"> CBOW</span><span class="pun">(</span><span class="pln">nn</span><span class="pun">.</span><span class="typ">Module</span><span class="pun">):</span></li><li class="L1"><span class="com"># Initialize the CBOW model</span></li><li class="L2"><span class="kwd">def</span><span class="pln"> __init__</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> vocab_size</span><span class="pun">,</span><span class="pln"> embed_dim</span><span class="pun">,</span><span class="pln"> num_class</span><span class="pun">):</span></li><li class="L3"><span class="kwd">super</span><span class="pun">(</span><span class="pln">CBOW</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">).</span><span class="pln">__init__</span><span class="pun">()</span></li><li class="L4"><span class="com"># Define the embedding layer using nn.EmbeddingBag</span></li><li class="L5"><span class="kwd">self</span><span class="pun">.</span><span class="pln">embedding </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">EmbeddingBag</span><span class="pun">(</span><span class="pln">vocab_size</span><span class="pun">,</span><span class="pln"> embed_dim</span><span class="pun">,</span><span class="pln"> sparse</span><span class="pun">=</span><span class="kwd">False</span><span class="pun">)</span></li><li class="L6"><span class="com"># Define the fully connected layer</span></li><li class="L7"><span class="kwd">self</span><span class="pun">.</span><span class="pln">fc </span><span class="pun">=</span><span class="pln"> nn</span><span class="pun">.</span><span class="typ">Linear</span><span class="pun">(</span><span class="pln">embed_dim</span><span class="pun">,</span><span class="pln"> vocab_size</span><span class="pun">)</span></li><li class="L8"><span class="kwd">def</span><span class="pln"> forward</span><span class="pun">(</span><span class="kwd">self</span><span class="pun">,</span><span class="pln"> text</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">):</span></li><li class="L9"><span class="com"># Pass the input text and offsets through the embedding layer</span></li><li class="L0"><span class="kwd">out</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">embedding</span><span class="pun">(</span><span class="pln">text</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">)</span></li><li class="L1"><span class="com"># Apply the ReLU activation function to the output of the first linear layer</span></li><li class="L2"><span class="kwd">out</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> torch</span><span class="pun">.</span><span class="pln">relu</span><span class="pun">(</span><span class="kwd">out</span><span class="pun">)</span></li><li class="L3"><span class="com"># Pass the output of the ReLU activation through the fully connected layer</span></li><li class="L4"><span class="kwd">return</span><span class="pln"> </span><span class="kwd">self</span><span class="pun">.</span><span class="pln">fc</span><span class="pun">(</span><span class="kwd">out</span><span class="pun">)</span></li><li class="L5"><span class="pln">vocab_size </span><span class="pun">=</span><span class="pln"> len</span><span class="pun">(</span><span class="pln">vocab</span><span class="pun">)</span></li><li class="L6"><span class="pln">emsize </span><span class="pun">=</span><span class="pln"> </span><span class="lit">24</span></li><li class="L7"><span class="pln">model_cbow </span><span class="pun">=</span><span class="pln"> CBOW</span><span class="pun">(</span><span class="pln">vocab_size</span><span class="pun">,</span><span class="pln"> emsize</span><span class="pun">,</span><span class="pln"> vocab_size</span><span class="pun">).</span><span class="pln">to</span><span class="pun">(</span><span class="pln">device</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-15">Copied!</span></button></pre>
          </td>
        </tr>
        <tr class="odd">
          <td>Training loop</td>
          <td>
            Enumerates data from the DataLoader and, on each pass of the loop,
            gets a batch of training data from the DataLoader, zeros the
            optimizer's gradients, and performs an inference (gets predictions
            from the model for an input batch).
          </td>
          <td>
            <pre
              class="prettyprint linenums prettyprinted"
              style=""
            ><ol class="formatted-line-numbers"><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li><li>6</li><li>7</li><li>8</li><li>9</li><li>10</li><li>11</li><li>12</li><li>13</li><li>14</li><li>15</li><li>16</li><li>17</li></ol><ol class="linenums"><li class="L0"><span class="kwd">for</span><span class="pln"> epoch </span><span class="kwd">in</span><span class="pln"> tqdm</span><span class="pun">(</span><span class="pln">range</span><span class="pun">(</span><span class="lit">1</span><span class="pun">,</span><span class="pln"> EPOCHS </span><span class="pun">+</span><span class="pln"> </span><span class="lit">1</span><span class="pun">)):</span></li><li class="L1"><span class="pln">    model</span><span class="pun">.</span><span class="pln">train</span><span class="pun">()</span></li><li class="L2"><span class="pln">    cum_loss</span><span class="pun">=</span><span class="lit">0</span></li><li class="L3"><span class="pln">    </span><span class="kwd">for</span><span class="pln"> idx</span><span class="pun">,</span><span class="pln"> </span><span class="pun">(</span><span class="pln">label</span><span class="pun">,</span><span class="pln"> text</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">)</span><span class="pln"> </span><span class="kwd">in</span><span class="pln"> enumerate</span><span class="pun">(</span><span class="pln">train_dataloader</span><span class="pun">):</span></li><li class="L4"><span class="pln">        optimizer</span><span class="pun">.</span><span class="pln">zero_grad</span><span class="pun">()</span></li><li class="L5"><span class="pln">        predicted_label </span><span class="pun">=</span><span class="pln"> model</span><span class="pun">(</span><span class="pln">text</span><span class="pun">,</span><span class="pln"> offsets</span><span class="pun">)</span></li><li class="L6"><span class="pln">        loss </span><span class="pun">=</span><span class="pln"> criterion</span><span class="pun">(</span><span class="pln">predicted_label</span><span class="pun">,</span><span class="pln"> label</span><span class="pun">)</span></li><li class="L7"><span class="pln">        loss</span><span class="pun">.</span><span class="pln">backward</span><span class="pun">()</span></li><li class="L8"><span class="pln">        torch</span><span class="pun">.</span><span class="pln">nn</span><span class="pun">.</span><span class="pln">utils</span><span class="pun">.</span><span class="pln">clip_grad_norm_</span><span class="pun">(</span><span class="pln">model</span><span class="pun">.</span><span class="pln">parameters</span><span class="pun">(),</span><span class="pln"> </span><span class="lit">0.1</span><span class="pun">)</span></li><li class="L9"><span class="pln">        optimizer</span><span class="pun">.</span><span class="pln">step</span><span class="pun">()</span></li><li class="L0"><span class="pln">        cum_loss</span><span class="pun">+=</span><span class="pln">loss</span><span class="pun">.</span><span class="pln">item</span><span class="pun">()</span></li><li class="L1"><span class="pln">    cum_loss_list</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">cum_loss</span><span class="pun">)</span></li><li class="L2"><span class="pln">    accu_val </span><span class="pun">=</span><span class="pln"> evaluate</span><span class="pun">(</span><span class="pln">valid_dataloader</span><span class="pun">)</span></li><li class="L3"><span class="pln">    acc_epoch</span><span class="pun">.</span><span class="pln">append</span><span class="pun">(</span><span class="pln">accu_val</span><span class="pun">)</span></li><li class="L4"><span class="pln">    </span><span class="kwd">if</span><span class="pln"> accu_val </span><span class="pun">&gt;</span><span class="pln"> acc_old</span><span class="pun">:</span></li><li class="L5"><span class="pln">        acc_old</span><span class="pun">=</span><span class="pln"> accu_val</span></li><li class="L6"><span class="pln">        torch</span><span class="pun">.</span><span class="pln">save</span><span class="pun">(</span><span class="pln">model</span><span class="pun">.</span><span class="pln">state_dict</span><span class="pun">(),</span><span class="pln"> </span><span class="str">'my_model.pth'</span><span class="pun">)</span></li></ol><button title="Copy" class="action-code-block copy-code-block multiple-lines"><i class="fa fa-copy" aria-hidden="true"></i><span class="popuptext" id="md-code-block-copy-16">Copied!</span></button></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <p>
      <img
        src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/footer_logo_sn_ibm.png"
        alt=""
      />
    </p>
  </body>
</html>
